{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T14:38:55.839644Z",
     "start_time": "2020-09-25T14:38:54.724479Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from utils import utils\n",
    "from utils.readers import InHospitalMortalityReader\n",
    "from utils.preprocessing import Discretizer, Normalizer\n",
    "from utils import metrics\n",
    "from utils import common_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T14:38:55.845071Z",
     "start_time": "2020-09-25T14:38:55.842137Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = './data/'\n",
    "small_part = False\n",
    "arg_timestep = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T14:39:05.708338Z",
     "start_time": "2020-09-25T14:38:55.847545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32268\n",
      "4033\n",
      "4035\n",
      "0.07267261683401513\n",
      "0.07265063228365981\n",
      "0.07286245353159851\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/Challenge/normalized/'\n",
    "\n",
    "train_x = pickle.load(open(data_path + 'train_x.dat', 'rb'))\n",
    "train_y = pickle.load(open(data_path + 'train_y.dat', 'rb'))\n",
    "train_x_len = pickle.load(open(data_path + 'train_x_len.dat', 'rb'))\n",
    "\n",
    "\n",
    "dev_x = pickle.load(open(data_path + 'dev_x.dat', 'rb'))\n",
    "dev_y = pickle.load(open(data_path + 'dev_y.dat', 'rb'))\n",
    "dev_x_len = pickle.load(open(data_path + 'dev_x_len.dat', 'rb'))\n",
    "\n",
    "test_x = pickle.load(open(data_path + 'test_x.dat', 'rb'))\n",
    "test_y = pickle.load(open(data_path + 'test_y.dat', 'rb'))\n",
    "test_x_len = pickle.load(open(data_path + 'test_x_len.dat', 'rb'))\n",
    "\n",
    "\n",
    "print(len(train_x))\n",
    "print(len(dev_x))\n",
    "print(len(test_x))\n",
    "print(sum(train_y)/len(train_y))\n",
    "print(sum(dev_y)/len(dev_y))\n",
    "print(sum(test_y)/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T14:39:05.720523Z",
     "start_time": "2020-09-25T14:39:05.710787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu')\n",
    "# device = torch.device('cuda')\n",
    "print(\"available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T14:39:05.782045Z",
     "start_time": "2020-09-25T14:39:05.722641Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_loss(y_pred, y_true):\n",
    "    loss = torch.nn.BCELoss()\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T14:39:05.863894Z",
     "start_time": "2020-09-25T14:39:05.784812Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_re_loss(y_pred, y_true):\n",
    "    loss = torch.nn.MSELoss()\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T14:39:05.924334Z",
     "start_time": "2020-09-25T14:39:05.867023Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_sents(sents, pad_token):\n",
    "\n",
    "    sents_padded = []\n",
    "\n",
    "    max_length = max([len(_) for _ in sents])\n",
    "    for i in sents:\n",
    "        padded = list(i) + [pad_token]*(max_length-len(i))\n",
    "        sents_padded.append(np.array(padded))\n",
    "\n",
    "\n",
    "    return np.array(sents_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T14:39:05.973956Z",
     "start_time": "2020-09-25T14:39:05.929031Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_iter(x, y, lens, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(x) / batch_size) \n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size] # Â fetch out all the induces\n",
    "        \n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx],  lens[idx]))\n",
    "       \n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    \n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "#         batch_name = [e[2] for e in examples]\n",
    "        batch_lens = [e[2] for e in examples]\n",
    "       \n",
    "\n",
    "        yield batch_x, batch_y, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T14:39:06.056269Z",
     "start_time": "2020-09-25T14:39:05.976743Z"
    }
   },
   "outputs": [],
   "source": [
    "def length_to_mask(length, max_len=None, dtype=None):\n",
    "    \"\"\"length: B.\n",
    "    return B x max_len.\n",
    "    If max_len is None, then max of length will be used.\n",
    "    \"\"\"\n",
    "    assert len(length.shape) == 1, 'Length shape should be 1 dimensional.'\n",
    "    max_len = max_len or length.max().item()\n",
    "    mask = torch.arange(max_len, device=length.device,\n",
    "                        dtype=length.dtype).expand(len(length), max_len) < length.unsqueeze(1)\n",
    "    if dtype is not None:\n",
    "        mask = torch.as_tensor(mask, dtype=dtype, device=length.device)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T14:39:06.132622Z",
     "start_time": "2020-09-25T14:39:06.059513Z"
    }
   },
   "outputs": [],
   "source": [
    "# K-NN cluster\n",
    "def random_init(dataset, num_centers):\n",
    "    num_points = dataset.size(0)\n",
    "    dimension = dataset.size(1)\n",
    "\n",
    "    indices = torch.tensor(np.array(random.sample(range(num_points), k=num_centers)), dtype=torch.long)\n",
    "\n",
    "    indices = indices.to(device)\n",
    "    centers = torch.gather(dataset, 0, indices.view(-1, 1).expand(-1, dimension))\n",
    "    return centers\n",
    "\n",
    "# Compute for each data point the closest center\n",
    "def compute_codes(dataset, centers):\n",
    "    num_points = dataset.size(0)\n",
    "    dimension = dataset.size(1)\n",
    "    num_centers = centers.size(0)\n",
    "    # 5e8 should vary depending on the free memory on the GPU\n",
    "    # Ideally, automatically ;)\n",
    "    chunk_size = int(5e8 / num_centers)\n",
    "    codes = torch.zeros(num_points, dtype=torch.long, device=device)\n",
    "    centers_t = torch.transpose(centers, 0, 1)\n",
    "    centers_norms = torch.sum(centers ** 2, dim=1).view(1, -1)\n",
    "    for i in range(0, num_points, chunk_size):\n",
    "        begin = i\n",
    "        end = min(begin + chunk_size, num_points)\n",
    "        dataset_piece = dataset[begin:end, :]\n",
    "        dataset_norms = torch.sum(dataset_piece ** 2, dim=1).view(-1, 1)\n",
    "        distances = torch.mm(dataset_piece, centers_t)\n",
    "        distances *= -2.0\n",
    "        distances += dataset_norms\n",
    "        distances += centers_norms\n",
    "        _, min_ind = torch.min(distances, dim=1)\n",
    "        codes[begin:end] = min_ind\n",
    "    return codes\n",
    "\n",
    "# Compute new centers as means of the data points forming the clusters\n",
    "def update_centers(dataset, codes, num_centers):\n",
    "    num_points = dataset.size(0)\n",
    "    dimension = dataset.size(1)\n",
    "    centers = torch.zeros(num_centers, dimension, dtype=torch.float, device=device)\n",
    "    cnt = torch.zeros(num_centers, dtype=torch.float, device=device)\n",
    "    centers.scatter_add_(0, codes.view(-1, 1).expand(-1, dimension), dataset)\n",
    "    cnt.scatter_add_(0, codes, torch.ones(num_points, dtype=torch.float, device=device))\n",
    "    # Avoiding division by zero\n",
    "    # Not necessary if there are no duplicates among the data points\n",
    "    cnt = torch.where(cnt > 0.5, cnt, torch.ones(num_centers, dtype=torch.float, device=device))\n",
    "    centers /= cnt.view(-1, 1)\n",
    "    return centers\n",
    "\n",
    "def cluster(dataset, num_centers):\n",
    "    centers = random_init(dataset, num_centers)\n",
    "    codes = compute_codes(dataset, centers)\n",
    "    num_iterations = 0\n",
    "    while True:\n",
    "#         sys.stdout.write('.')\n",
    "#         sys.stdout.flush()\n",
    "#         num_iterations += 1\n",
    "        centers = update_centers(dataset, codes, num_centers)\n",
    "        new_codes = compute_codes(dataset, centers)\n",
    "        # Waiting until the clustering stops updating altogether\n",
    "        # This is too strict in practice\n",
    "        if torch.equal(codes, new_codes):\n",
    "#             sys.stdout.write('\\n')\n",
    "#             print('Converged in %d iterations' % num_iterations)\n",
    "            break\n",
    "        if num_iterations>1000:\n",
    "            break\n",
    "        codes = new_codes\n",
    "    return centers, codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T14:39:06.222735Z",
     "start_time": "2020-09-25T14:39:06.135338Z"
    }
   },
   "outputs": [],
   "source": [
    "# the concare model as our backbone: https://www.aaai.org/ojs/index.php/AAAI/article/download/5428/5284\n",
    "class SingleAttention(nn.Module):\n",
    "    def __init__(self, attention_input_dim, attention_hidden_dim, attention_type='add', demographic_dim=12, time_aware=False, use_demographic=False):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        \n",
    "        self.attention_type = attention_type\n",
    "        self.attention_hidden_dim = attention_hidden_dim\n",
    "        self.attention_input_dim = attention_input_dim\n",
    "        self.use_demographic = use_demographic\n",
    "        self.demographic_dim = demographic_dim\n",
    "        self.time_aware = time_aware\n",
    "\n",
    "        \n",
    "        if attention_type == 'add':\n",
    "            if self.time_aware == True:\n",
    "                # self.Wx = nn.Parameter(torch.randn(attention_input_dim+1, attention_hidden_dim))\n",
    "                self.Wx = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "                self.Wtime_aware = nn.Parameter(torch.randn(1, attention_hidden_dim))\n",
    "                nn.init.kaiming_uniform_(self.Wtime_aware, a=math.sqrt(5))\n",
    "            else:\n",
    "                self.Wx = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "            self.Wt = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "            self.Wd = nn.Parameter(torch.randn(demographic_dim, attention_hidden_dim))\n",
    "            self.bh = nn.Parameter(torch.zeros(attention_hidden_dim,))\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wd, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wx, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wt, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        elif attention_type == 'mul':\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_input_dim, attention_input_dim))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        elif attention_type == 'concat':\n",
    "            if self.time_aware == True:\n",
    "                self.Wh = nn.Parameter(torch.randn(2*attention_input_dim+1, attention_hidden_dim))\n",
    "            else:\n",
    "                self.Wh = nn.Parameter(torch.randn(2*attention_input_dim, attention_hidden_dim))\n",
    "\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wh, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        else:\n",
    "            raise RuntimeError('Wrong attention type.')\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, input, demo=None):\n",
    " \n",
    "        batch_size, time_step, input_dim = input.size() # batch_size * time_step * hidden_dim(i)\n",
    "       \n",
    "        time_decays = torch.tensor(range(time_step-1,-1,-1), dtype=torch.float32).unsqueeze(-1).unsqueeze(0).to(device)# 1*t*1\n",
    "        b_time_decays = time_decays.repeat(batch_size,1,1)# b t 1\n",
    "        \n",
    "        if self.attention_type == 'add': #B*T*I  @ H*I\n",
    "            q = torch.matmul(input[:,-1,:], self.Wt)# b h\n",
    "            q = torch.reshape(q, (batch_size, 1, self.attention_hidden_dim)) #B*1*H\n",
    "            if self.time_aware == True:\n",
    "                # k_input = torch.cat((input, time), dim=-1)\n",
    "                k = torch.matmul(input, self.Wx)#b t h\n",
    "                # k = torch.reshape(k, (batch_size, 1, time_step, self.attention_hidden_dim)) #B*1*T*H\n",
    "                time_hidden = torch.matmul(b_time_decays, self.Wtime_aware)#  b t h\n",
    "            else:\n",
    "                k = torch.matmul(input, self.Wx)# b t h\n",
    "                # k = torch.reshape(k, (batch_size, 1, time_step, self.attention_hidden_dim)) #B*1*T*H\n",
    "            if self.use_demographic == True:\n",
    "                d = torch.matmul(demo, self.Wd) #B*H\n",
    "                d = torch.reshape(d, (batch_size, 1, self.attention_hidden_dim)) # b 1 h\n",
    "            h = q + k + self.bh # b t h\n",
    "            if self.time_aware == True:\n",
    "                h += time_hidden\n",
    "            h = self.tanh(h) #B*T*H\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step))# b t\n",
    "        elif self.attention_type == 'mul':\n",
    "            e = torch.matmul(input[:,-1,:], self.Wa)#b i\n",
    "            e = torch.matmul(e.unsqueeze(1), input.permute(0,2,1)).squeeze() + self.ba #b t\n",
    "        elif self.attention_type == 'concat':\n",
    "            q = input[:,-1,:].unsqueeze(1).repeat(1,time_step,1)# b t i\n",
    "            k = input\n",
    "            c = torch.cat((q, k), dim=-1) #B*T*2I\n",
    "            if self.time_aware == True:\n",
    "                c = torch.cat((c, b_time_decays), dim=-1) #B*T*2I+1\n",
    "            h = torch.matmul(c, self.Wh)\n",
    "            h = self.tanh(h)\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step)) # b t \n",
    "        \n",
    "   \n",
    "        a = self.softmax(e) #B*T\n",
    "        v = torch.matmul(a.unsqueeze(1), input).squeeze() #B*I\n",
    "\n",
    "        return v, a\n",
    "\n",
    "class FinalAttentionQKV(nn.Module):\n",
    "    def __init__(self, attention_input_dim, attention_hidden_dim, attention_type='add', dropout=None):\n",
    "        super(FinalAttentionQKV, self).__init__()\n",
    "        \n",
    "        self.attention_type = attention_type\n",
    "        self.attention_hidden_dim = attention_hidden_dim\n",
    "        self.attention_input_dim = attention_input_dim\n",
    "\n",
    "\n",
    "        self.W_q = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "        self.W_k = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "        self.W_v = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "\n",
    "        self.W_out = nn.Linear(attention_hidden_dim, 1)\n",
    "\n",
    "        self.b_in = nn.Parameter(torch.zeros(1,))\n",
    "        self.b_out = nn.Parameter(torch.zeros(1,))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.W_q.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_k.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_v.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_out.weight, a=math.sqrt(5))\n",
    "\n",
    "        self.Wh = nn.Parameter(torch.randn(2*attention_input_dim, attention_hidden_dim))\n",
    "        self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "        self.ba = nn.Parameter(torch.zeros(1,))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.Wh, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input):\n",
    " \n",
    "        batch_size, time_step, input_dim = input.size() # batch_size * input_dim + 1 * hidden_dim(i)\n",
    "        input_q = self.W_q(torch.mean(input,1)) # b h\n",
    "        input_k = self.W_k(input)# b t h\n",
    "        input_v = self.W_v(input)# b t h\n",
    "\n",
    "        if self.attention_type == 'add': #B*T*I  @ H*I\n",
    "\n",
    "            q = torch.reshape(input_q, (batch_size, 1, self.attention_hidden_dim)) #B*1*H\n",
    "            h = q + input_k + self.b_in # b t h\n",
    "            h = self.tanh(h) #B*T*H\n",
    "            e = self.W_out(h) # b t 1\n",
    "            e = torch.reshape(e, (batch_size, time_step))# b t\n",
    "\n",
    "        elif self.attention_type == 'mul':\n",
    "            q = torch.reshape(input_q, (batch_size, self.attention_hidden_dim, 1)) #B*h 1\n",
    "            e = torch.matmul(input_k, q).squeeze()#b t\n",
    "            \n",
    "        elif self.attention_type == 'concat':\n",
    "            q = input_q.unsqueeze(1).repeat(1,time_step,1)# b t h\n",
    "            k = input_k\n",
    "            c = torch.cat((q, k), dim=-1) #B*T*2I\n",
    "            h = torch.matmul(c, self.Wh)\n",
    "            h = self.tanh(h)\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step)) # b t \n",
    "        \n",
    "        a = self.softmax(e) #B*T\n",
    "        if self.dropout is not None:\n",
    "            a = self.dropout(a)\n",
    "        v = torch.matmul(a.unsqueeze(1), input_v).squeeze() #B*I\n",
    "\n",
    "        return v, a\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def tile(a, dim, n_tile):\n",
    "    init_dim = a.size(dim)\n",
    "    repeat_idx = [1] * a.dim()\n",
    "    repeat_idx[dim] = n_tile\n",
    "    a = a.repeat(*(repeat_idx))\n",
    "    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).to(device)\n",
    "    return torch.index_select(a, dim, order_index).to(device)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module): # new added\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x)))), None\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module): # new added / not use anymore\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=400):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0 \n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)# b h t d_k\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k) # b h t t\n",
    "    if mask is not None:# 1 1 t t\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)# b h t t \n",
    "    p_attn = F.softmax(scores, dim = -1)# b h t t\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn # b h t v (d_k) \n",
    "    \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, self.d_k * self.h), 3)\n",
    "        self.final_linear = nn.Linear(d_model, d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1) # 1 1 t t\n",
    "\n",
    "        nbatches = query.size(0)# b\n",
    "        input_dim = query.size(1)# i+1\n",
    "        feature_dim = query.size(-1)# i+1\n",
    "\n",
    "        #input size -> # batch_size * d_input * hidden_dim\n",
    "        \n",
    "        # d_model => h * d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))] # b num_head d_input d_k\n",
    "        \n",
    "       \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)# b num_head d_input d_v (d_k) \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)# batch_size * d_input * hidden_dim\n",
    "\n",
    "\n",
    "\n",
    "        return self.final_linear(x), torch.zeros(1).to(device)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-7):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "def cov(m, y=None):\n",
    "    if y is not None:\n",
    "        m = torch.cat((m, y), dim=0)\n",
    "    m_exp = torch.mean(m, dim=1)\n",
    "    x = m - m_exp[:, None]\n",
    "    cov = 1 / (x.size(1) - 1) * x.mm(x.t())\n",
    "    return cov\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        returned_value = sublayer(self.norm(x))\n",
    "        return x + self.dropout(returned_value[0]) , returned_value[1]\n",
    "\n",
    "class vanilla_transformer_encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(vanilla_transformer_encoder, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "#         self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 400)\n",
    "\n",
    "        self.GRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "#         self.LastStepAttentions = clones(SingleAttention(self.hidden_dim, 8, attention_type='concat', demographic_dim=12, time_aware=True, use_demographic=False),self.input_dim)\n",
    "        \n",
    "        self.FinalAttentionQKV = FinalAttentionQKV(self.hidden_dim, self.hidden_dim, attention_type='mul',dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model,dropout = 1 - self.keep_prob)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "\n",
    "        \n",
    "        self.to_query = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.to_key = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.to_value = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.output = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(self.hidden_dim)\n",
    "\n",
    "    def forward(self, input, lens):\n",
    "\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "\n",
    "       \n",
    "        GRU_embeded_input = self.GRUs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1].squeeze(0).unsqueeze(1) # b 1 h\n",
    "\n",
    "        for i in range(feature_dim-1):\n",
    "            embeded_input = self.GRUs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1].squeeze(0).unsqueeze(1) # b 1 h\n",
    "            GRU_embeded_input = torch.cat((GRU_embeded_input, embeded_input), 1)\n",
    "\n",
    "        posi_input = self.dropout(GRU_embeded_input) # batch_size * d_input * hidden_dim\n",
    "\n",
    "        contexts = self.SublayerConnection(posi_input, lambda x: self.MultiHeadedAttention(self.tanh(self.to_query(posi_input)), self.tanh(self.to_key(posi_input)), self.tanh(self.to_value(posi_input)), None))# # batch_size * d_input * hidden_dim\n",
    "    \n",
    "        DeCov_loss = contexts[1]\n",
    "        contexts = contexts[0]\n",
    "\n",
    "        contexts = self.SublayerConnection(contexts, lambda x: self.PositionwiseFeedForward(contexts))[0]# # batch_size * d_input * hidden_dim\n",
    "      \n",
    "\n",
    "        weighted_contexts = self.FinalAttentionQKV(contexts)[0]\n",
    "        output = self.output(self.dropout(self.bn(weighted_contexts)))# b 1\n",
    "        output = self.sigmoid(output)\n",
    "          \n",
    "        return output, DeCov_loss  ,weighted_contexts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T14:39:06.310214Z",
     "start_time": "2020-09-25T14:39:06.224853Z"
    }
   },
   "outputs": [],
   "source": [
    "# Our MAPLE framework\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(in_features, out_features).float())\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features).float())\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        std = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-std, std)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, adj, x):\n",
    "        y = torch.mm(x.float(), self.weight.float())\n",
    "        output = torch.mm(adj.float(), y.float())\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias.float()\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "class MAPLE(nn.Module):\n",
    "    def __init__(self, device, input_dim = 76, hidden_dim = 32, output_dim = 1, cluster_num = 12, dropout=0.0, block='LSTM'):\n",
    "        super(MAPLE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.cluster_num = cluster_num\n",
    "        self.dropout = dropout\n",
    "        self.block = block\n",
    "\n",
    "        \n",
    "        self.embed_layer = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.GRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        self.opt_layer2 = nn.Linear( self.input_dim*self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        self.opt_layer = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "        \n",
    "        nn.init.xavier_uniform_(self.embed_layer.weight)\n",
    "        nn.init.xavier_uniform_(self.opt_layer.weight)\n",
    "        \n",
    "        self.backbone = vanilla_transformer_encoder(input_dim = self.input_dim, hidden_dim = self.hidden_dim, d_model = self.hidden_dim,  MHD_num_head = 4 , d_ff = 2*self.hidden_dim, output_dim = self.output_dim).to(device)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.bn1 = nn.BatchNorm1d(2 * self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.weight1 = nn.Linear(self.hidden_dim, 1)\n",
    "        self.weight2 = nn.Linear(self.hidden_dim, 1)\n",
    "        self.GCN = GraphConvolution(self.hidden_dim, self.hidden_dim, bias=True)\n",
    "        self.GCN.initialize_parameters()\n",
    "        self.GCN_2 = GraphConvolution(self.hidden_dim, self.hidden_dim, bias=True)\n",
    "        self.GCN_2.initialize_parameters()\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(self.hidden_dim)\n",
    "\n",
    "        \n",
    "    def sample_gumbel(self, shape, eps=1e-20):\n",
    "        U = torch.rand(shape).to(device)\n",
    "\n",
    "        return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, temperature):\n",
    "        y = logits + self.sample_gumbel(logits.size())\n",
    "        return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "\n",
    "    def gumbel_softmax(self, logits, temperature, hard=False):\n",
    "        \"\"\"\n",
    "        ST-gumple-softmax\n",
    "        input: [*, n_class]\n",
    "        return: flatten --> [*, n_class] an one-hot vector\n",
    "        \"\"\"\n",
    "        y = self.gumbel_softmax_sample(logits, temperature)\n",
    "\n",
    "        if not hard:\n",
    "            return y.view(-1, self.cluster_num)\n",
    "\n",
    "        shape = y.size()\n",
    "        _, ind = y.max(dim=-1)\n",
    "        y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "        y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "        y_hard = y_hard.view(*shape)\n",
    "        # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
    "        y_hard = (y_hard - y).detach() + y\n",
    "        return y_hard\n",
    "    \n",
    "\n",
    "        \n",
    "    def forward(self, input, epoch, lens):\n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        \n",
    " \n",
    "        _,_, hidden_t = self.backbone(input,lens)\n",
    "        \n",
    "        centers, codes = cluster(hidden_t, self.cluster_num)\n",
    "        \n",
    "        if epoch == 0:\n",
    "            A_mat = np.eye(self.cluster_num)\n",
    "        else:\n",
    "            A_mat = kneighbors_graph(np.array(centers.detach().cpu().numpy()), 20, mode='connectivity', include_self=False).toarray()\n",
    "        \n",
    "        adj_mat = torch.tensor(A_mat).to(device)\n",
    "        \n",
    "        e = self.relu(torch.matmul(hidden_t, centers.transpose(0, 1)) )# b clu_num\n",
    "\n",
    "        scores = self.gumbel_softmax(e, temperature = 1, hard = True)\n",
    "        digits = torch.argmax(scores, dim = -1)#  b\n",
    "\n",
    "\n",
    "        h_prime = self.relu(self.GCN(adj_mat, centers))\n",
    "        h_prime = self.relu(self.GCN_2(adj_mat, h_prime))\n",
    "\n",
    "\n",
    "        clu_appendix = torch.matmul(scores, h_prime)\n",
    "         \n",
    "        weight1=torch.sigmoid(self.weight1(clu_appendix))\n",
    "        weight2 = torch.sigmoid(self.weight2(hidden_t ))\n",
    "        weight1 = weight1/(weight1+weight2)\n",
    "        weight2= 1-weight1\n",
    "\n",
    "        final_h = weight1*clu_appendix+weight2*hidden_t\n",
    "\n",
    "\n",
    "\n",
    "        opt = self.sigmoid(self.opt_layer(self.bn(final_h)))\n",
    "        \n",
    "                      \n",
    "        return opt, hidden_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T14:39:13.076386Z",
     "start_time": "2020-09-25T14:39:06.312814Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 2000\n",
    "np.random.seed(RANDOM_SEED) #numpy\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED) # cpu\n",
    "torch.cuda.manual_seed(RANDOM_SEED) #gpu\n",
    "torch.backends.cudnn.deterministic=True # cudnn\n",
    "\n",
    "input_dim = 33\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "cluster_num = 100\n",
    "dropout = 0.5\n",
    "block = 'GRU'\n",
    "\n",
    "\n",
    "model = MAPLE(device = device, input_dim = input_dim, hidden_dim = hidden_dim, output_dim = output_dim, cluster_num = cluster_num, dropout=dropout, block=block).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-25T14:38:58.037Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0: Train Loss = 0.7079\n",
      "Epoch 0 Batch 30: Train Loss = 0.6272\n",
      "Epoch 0 Batch 60: Train Loss = 0.5758\n",
      "Epoch 0 Batch 90: Train Loss = 0.5293\n",
      "Epoch 0 Batch 120: Train Loss = 0.4884\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 6.1226\n",
      "valid_model Loss = 6.1226\n",
      "confusion matrix:\n",
      "[[3740    0]\n",
      " [ 293    0]]\n",
      "accuracy = 0.9273493885993958\n",
      "precision class 0 = 0.9273493885993958\n",
      "precision class 1 = nan\n",
      "recall class 0 = 1.0\n",
      "recall class 1 = 0.0\n",
      "AUC of ROC = 0.5432069135441953\n",
      "AUC of PRC = 0.0913012870160394\n",
      "min(+P, Se) = 0.1346153846153846\n",
      "f1_score = nan\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/other/choczhang/EMR/challenge/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0: Train Loss = 0.3684\n",
      "Epoch 1 Batch 30: Train Loss = 0.3072\n",
      "Epoch 1 Batch 60: Train Loss = 0.2882\n",
      "Epoch 1 Batch 90: Train Loss = 0.2753\n",
      "Epoch 1 Batch 120: Train Loss = 0.2692\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.2707\n",
      "valid_model Loss = 0.2707\n",
      "confusion matrix:\n",
      "[[3583  157]\n",
      " [ 230   63]]\n",
      "accuracy = 0.9040416479110718\n",
      "precision class 0 = 0.9396800398826599\n",
      "precision class 1 = 0.2863636314868927\n",
      "recall class 0 = 0.9580214023590088\n",
      "recall class 1 = 0.21501706540584564\n",
      "AUC of ROC = 0.7805378620576371\n",
      "AUC of PRC = 0.22781390046352282\n",
      "min(+P, Se) = 0.2721311475409836\n",
      "f1_score = 0.2456140409586306\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 2 Batch 0: Train Loss = 0.1857\n",
      "Epoch 2 Batch 30: Train Loss = 0.2282\n",
      "Epoch 2 Batch 60: Train Loss = 0.2260\n",
      "Epoch 2 Batch 90: Train Loss = 0.2226\n",
      "Epoch 2 Batch 120: Train Loss = 0.2175\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.2203\n",
      "valid_model Loss = 0.2203\n",
      "confusion matrix:\n",
      "[[3631  109]\n",
      " [ 207   86]]\n",
      "accuracy = 0.9216464161872864\n",
      "precision class 0 = 0.9460656642913818\n",
      "precision class 1 = 0.44102564454078674\n",
      "recall class 0 = 0.9708555936813354\n",
      "recall class 1 = 0.2935153543949127\n",
      "AUC of ROC = 0.8333079337847457\n",
      "AUC of PRC = 0.3650930224616201\n",
      "min(+P, Se) = 0.4300341296928328\n",
      "f1_score = 0.35245901465594154\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 3 Batch 0: Train Loss = 0.1717\n",
      "Epoch 3 Batch 30: Train Loss = 0.1995\n",
      "Epoch 3 Batch 60: Train Loss = 0.2011\n",
      "Epoch 3 Batch 90: Train Loss = 0.2045\n",
      "Epoch 3 Batch 120: Train Loss = 0.2012\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1868\n",
      "valid_model Loss = 0.1868\n",
      "confusion matrix:\n",
      "[[3716   24]\n",
      " [ 233   60]]\n",
      "accuracy = 0.9362757205963135\n",
      "precision class 0 = 0.9409977197647095\n",
      "precision class 1 = 0.7142857313156128\n",
      "recall class 0 = 0.9935829043388367\n",
      "recall class 1 = 0.20477814972400665\n",
      "AUC of ROC = 0.8606157945648009\n",
      "AUC of PRC = 0.4683597669577438\n",
      "min(+P, Se) = 0.48464163822525597\n",
      "f1_score = 0.31830238533397925\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 4 Batch 0: Train Loss = 0.2130\n",
      "Epoch 4 Batch 30: Train Loss = 0.1865\n",
      "Epoch 4 Batch 60: Train Loss = 0.1830\n",
      "Epoch 4 Batch 90: Train Loss = 0.1850\n",
      "Epoch 4 Batch 120: Train Loss = 0.1838\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1871\n",
      "valid_model Loss = 0.1871\n",
      "confusion matrix:\n",
      "[[3711   29]\n",
      " [ 221   72]]\n",
      "accuracy = 0.9380114078521729\n",
      "precision class 0 = 0.9437944889068604\n",
      "precision class 1 = 0.7128713130950928\n",
      "recall class 0 = 0.9922459721565247\n",
      "recall class 1 = 0.24573378264904022\n",
      "AUC of ROC = 0.9038117574054133\n",
      "AUC of PRC = 0.5705368018846206\n",
      "min(+P, Se) = 0.5898305084745763\n",
      "f1_score = 0.3654822248776113\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 5 Batch 0: Train Loss = 0.1260\n",
      "Epoch 5 Batch 30: Train Loss = 0.1604\n",
      "Epoch 5 Batch 60: Train Loss = 0.1535\n",
      "Epoch 5 Batch 90: Train Loss = 0.1572\n",
      "Epoch 5 Batch 120: Train Loss = 0.1576\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1568\n",
      "valid_model Loss = 0.1568\n",
      "confusion matrix:\n",
      "[[3695   45]\n",
      " [ 149  144]]\n",
      "accuracy = 0.9518968462944031\n",
      "precision class 0 = 0.9612382650375366\n",
      "precision class 1 = 0.761904776096344\n",
      "recall class 0 = 0.9879679083824158\n",
      "recall class 1 = 0.49146756529808044\n",
      "AUC of ROC = 0.9075733240860726\n",
      "AUC of PRC = 0.660309523126519\n",
      "min(+P, Se) = 0.6054421768707483\n",
      "f1_score = 0.5975103835210793\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 6 Batch 0: Train Loss = 0.1745\n",
      "Epoch 6 Batch 30: Train Loss = 0.1543\n",
      "Epoch 6 Batch 60: Train Loss = 0.1496\n",
      "Epoch 6 Batch 90: Train Loss = 0.1498\n",
      "Epoch 6 Batch 120: Train Loss = 0.1496\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1427\n",
      "valid_model Loss = 0.1427\n",
      "confusion matrix:\n",
      "[[3692   48]\n",
      " [ 137  156]]\n",
      "accuracy = 0.9541284441947937\n",
      "precision class 0 = 0.964220404624939\n",
      "precision class 1 = 0.7647058963775635\n",
      "recall class 0 = 0.9871657490730286\n",
      "recall class 1 = 0.532423198223114\n",
      "AUC of ROC = 0.9209158438429669\n",
      "AUC of PRC = 0.6875285161949133\n",
      "min(+P, Se) = 0.6394557823129252\n",
      "f1_score = 0.6277665685478353\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 7 Batch 0: Train Loss = 0.1612\n",
      "Epoch 7 Batch 30: Train Loss = 0.1546\n",
      "Epoch 7 Batch 60: Train Loss = 0.1488\n",
      "Epoch 7 Batch 90: Train Loss = 0.1467\n",
      "Epoch 7 Batch 120: Train Loss = 0.1467\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1422\n",
      "valid_model Loss = 0.1422\n",
      "confusion matrix:\n",
      "[[3703   37]\n",
      " [ 149  144]]\n",
      "accuracy = 0.9538804888725281\n",
      "precision class 0 = 0.9613187909126282\n",
      "precision class 1 = 0.7955800890922546\n",
      "recall class 0 = 0.9901069402694702\n",
      "recall class 1 = 0.49146756529808044\n",
      "AUC of ROC = 0.9183688927013562\n",
      "AUC of PRC = 0.6896095470299393\n",
      "min(+P, Se) = 0.6382252559726962\n",
      "f1_score = 0.6075949357522198\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 8 Batch 0: Train Loss = 0.1450\n",
      "Epoch 8 Batch 30: Train Loss = 0.1517\n",
      "Epoch 8 Batch 60: Train Loss = 0.1472\n",
      "Epoch 8 Batch 90: Train Loss = 0.1443\n",
      "Epoch 8 Batch 120: Train Loss = 0.1412\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1442\n",
      "valid_model Loss = 0.1442\n",
      "confusion matrix:\n",
      "[[3673   67]\n",
      " [ 123  170]]\n",
      "accuracy = 0.9528886675834656\n",
      "precision class 0 = 0.967597484588623\n",
      "precision class 1 = 0.7172995805740356\n",
      "recall class 0 = 0.9820855855941772\n",
      "recall class 1 = 0.5802047848701477\n",
      "AUC of ROC = 0.920391122629629\n",
      "AUC of PRC = 0.6934531299863435\n",
      "min(+P, Se) = 0.6348122866894198\n",
      "f1_score = 0.6415094096018111\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 9 Batch 0: Train Loss = 0.1420\n",
      "Epoch 9 Batch 30: Train Loss = 0.1390\n",
      "Epoch 9 Batch 60: Train Loss = 0.1381\n",
      "Epoch 9 Batch 90: Train Loss = 0.1400\n",
      "Epoch 9 Batch 120: Train Loss = 0.1400\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1330\n",
      "valid_model Loss = 0.1330\n",
      "confusion matrix:\n",
      "[[3685   55]\n",
      " [ 125  168]]\n",
      "accuracy = 0.9553682208061218\n",
      "precision class 0 = 0.9671915769577026\n",
      "precision class 1 = 0.7533632516860962\n",
      "recall class 0 = 0.9852941036224365\n",
      "recall class 1 = 0.57337886095047\n",
      "AUC of ROC = 0.9315672281944114\n",
      "AUC of PRC = 0.719021306248043\n",
      "min(+P, Se) = 0.658703071672355\n",
      "f1_score = 0.6511627838045119\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 10 Batch 0: Train Loss = 0.1285\n",
      "Epoch 10 Batch 30: Train Loss = 0.1328\n",
      "Epoch 10 Batch 60: Train Loss = 0.1351\n",
      "Epoch 10 Batch 90: Train Loss = 0.1332\n",
      "Epoch 10 Batch 120: Train Loss = 0.1349\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1299\n",
      "valid_model Loss = 0.1299\n",
      "confusion matrix:\n",
      "[[3690   50]\n",
      " [ 124  169]]\n",
      "accuracy = 0.9568559527397156\n",
      "precision class 0 = 0.9674882292747498\n",
      "precision class 1 = 0.7716894745826721\n",
      "recall class 0 = 0.9866310358047485\n",
      "recall class 1 = 0.5767918229103088\n",
      "AUC of ROC = 0.932307313244876\n",
      "AUC of PRC = 0.7250622353919678\n",
      "min(+P, Se) = 0.6666666666666666\n",
      "f1_score = 0.6601562507285048\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 11 Batch 0: Train Loss = 0.0708\n",
      "Epoch 11 Batch 30: Train Loss = 0.1422\n",
      "Epoch 11 Batch 60: Train Loss = 0.1342\n",
      "Epoch 11 Batch 90: Train Loss = 0.1374\n",
      "Epoch 11 Batch 120: Train Loss = 0.1367\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1284\n",
      "valid_model Loss = 0.1284\n",
      "confusion matrix:\n",
      "[[3690   50]\n",
      " [ 125  168]]\n",
      "accuracy = 0.95660799741745\n",
      "precision class 0 = 0.9672346115112305\n",
      "precision class 1 = 0.7706422209739685\n",
      "recall class 0 = 0.9866310358047485\n",
      "recall class 1 = 0.57337886095047\n",
      "AUC of ROC = 0.9364147396470223\n",
      "AUC of PRC = 0.7278958500164272\n",
      "min(+P, Se) = 0.6518771331058021\n",
      "f1_score = 0.6575342675870862\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 12 Batch 0: Train Loss = 0.1219\n",
      "Epoch 12 Batch 30: Train Loss = 0.1268\n",
      "Epoch 12 Batch 60: Train Loss = 0.1341\n",
      "Epoch 12 Batch 90: Train Loss = 0.1367\n",
      "Epoch 12 Batch 120: Train Loss = 0.1350\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1325\n",
      "valid_model Loss = 0.1325\n",
      "confusion matrix:\n",
      "[[3694   46]\n",
      " [ 127  166]]\n",
      "accuracy = 0.9571039080619812\n",
      "precision class 0 = 0.9667626023292542\n",
      "precision class 1 = 0.7830188870429993\n",
      "recall class 0 = 0.9877005219459534\n",
      "recall class 1 = 0.5665528774261475\n",
      "AUC of ROC = 0.9367423481958715\n",
      "AUC of PRC = 0.7318020634288803\n",
      "min(+P, Se) = 0.6689419795221843\n",
      "f1_score = 0.6574257624610924\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 13 Batch 0: Train Loss = 0.0959\n",
      "Epoch 13 Batch 30: Train Loss = 0.1274\n",
      "Epoch 13 Batch 60: Train Loss = 0.1250\n",
      "Epoch 13 Batch 90: Train Loss = 0.1314\n",
      "Epoch 13 Batch 120: Train Loss = 0.1321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1331\n",
      "valid_model Loss = 0.1331\n",
      "confusion matrix:\n",
      "[[3714   26]\n",
      " [ 145  148]]\n",
      "accuracy = 0.9575998187065125\n",
      "precision class 0 = 0.9624254703521729\n",
      "precision class 1 = 0.8505747318267822\n",
      "recall class 0 = 0.9930481314659119\n",
      "recall class 1 = 0.5051194429397583\n",
      "AUC of ROC = 0.9383712653537992\n",
      "AUC of PRC = 0.7364275044433395\n",
      "min(+P, Se) = 0.6791808873720137\n",
      "f1_score = 0.6338329731231097\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 14 Batch 0: Train Loss = 0.1003\n",
      "Epoch 14 Batch 30: Train Loss = 0.1206\n",
      "Epoch 14 Batch 60: Train Loss = 0.1249\n",
      "Epoch 14 Batch 90: Train Loss = 0.1308\n",
      "Epoch 14 Batch 120: Train Loss = 0.1307\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1326\n",
      "valid_model Loss = 0.1326\n",
      "confusion matrix:\n",
      "[[3708   32]\n",
      " [ 139  154]]\n",
      "accuracy = 0.9575998187065125\n",
      "precision class 0 = 0.9638679623603821\n",
      "precision class 1 = 0.8279569745063782\n",
      "recall class 0 = 0.9914438724517822\n",
      "recall class 1 = 0.5255972743034363\n",
      "AUC of ROC = 0.9342912157106095\n",
      "AUC of PRC = 0.7408168119003594\n",
      "min(+P, Se) = 0.6912751677852349\n",
      "f1_score = 0.6430062621039687\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 15 Batch 0: Train Loss = 0.1396\n",
      "Epoch 15 Batch 30: Train Loss = 0.1285\n",
      "Epoch 15 Batch 60: Train Loss = 0.1301\n",
      "Epoch 15 Batch 90: Train Loss = 0.1295\n",
      "Epoch 15 Batch 120: Train Loss = 0.1293\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1234\n",
      "valid_model Loss = 0.1234\n",
      "confusion matrix:\n",
      "[[3670   70]\n",
      " [ 107  186]]\n",
      "accuracy = 0.9561120867729187\n",
      "precision class 0 = 0.9716706275939941\n",
      "precision class 1 = 0.7265625\n",
      "recall class 0 = 0.98128342628479\n",
      "recall class 1 = 0.6348122954368591\n",
      "AUC of ROC = 0.9411737329123396\n",
      "AUC of PRC = 0.7526431391262212\n",
      "min(+P, Se) = 0.689419795221843\n",
      "f1_score = 0.6775956037314668\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 16 Batch 0: Train Loss = 0.1733\n",
      "Epoch 16 Batch 30: Train Loss = 0.1283\n",
      "Epoch 16 Batch 60: Train Loss = 0.1283\n",
      "Epoch 16 Batch 90: Train Loss = 0.1272\n",
      "Epoch 16 Batch 120: Train Loss = 0.1274\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1215\n",
      "valid_model Loss = 0.1215\n",
      "confusion matrix:\n",
      "[[3685   55]\n",
      " [ 115  178]]\n",
      "accuracy = 0.9578477740287781\n",
      "precision class 0 = 0.9697368144989014\n",
      "precision class 1 = 0.7639485001564026\n",
      "recall class 0 = 0.9852941036224365\n",
      "recall class 1 = 0.6075085401535034\n",
      "AUC of ROC = 0.9417422569400085\n",
      "AUC of PRC = 0.7493878472189038\n",
      "min(+P, Se) = 0.6996587030716723\n",
      "f1_score = 0.6768060599363858\n",
      "\n",
      "Epoch 17 Batch 0: Train Loss = 0.1083\n",
      "Epoch 17 Batch 30: Train Loss = 0.1236\n",
      "Epoch 17 Batch 60: Train Loss = 0.1286\n",
      "Epoch 17 Batch 90: Train Loss = 0.1269\n",
      "Epoch 17 Batch 120: Train Loss = 0.1273\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1308\n",
      "valid_model Loss = 0.1308\n",
      "confusion matrix:\n",
      "[[3668   72]\n",
      " [ 102  191]]\n",
      "accuracy = 0.9568559527397156\n",
      "precision class 0 = 0.9729443192481995\n",
      "precision class 1 = 0.7262357473373413\n",
      "recall class 0 = 0.9807486534118652\n",
      "recall class 1 = 0.6518771052360535\n",
      "AUC of ROC = 0.9427962621598438\n",
      "AUC of PRC = 0.7491210102544771\n",
      "min(+P, Se) = 0.6802721088435374\n",
      "f1_score = 0.6870503765854572\n",
      "\n",
      "Epoch 18 Batch 0: Train Loss = 0.1360\n",
      "Epoch 18 Batch 30: Train Loss = 0.1347\n",
      "Epoch 18 Batch 60: Train Loss = 0.1307\n",
      "Epoch 18 Batch 90: Train Loss = 0.1294\n",
      "Epoch 18 Batch 120: Train Loss = 0.1275\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1306\n",
      "valid_model Loss = 0.1306\n",
      "confusion matrix:\n",
      "[[3706   34]\n",
      " [ 132  161]]\n",
      "accuracy = 0.9588395953178406\n",
      "precision class 0 = 0.9656071066856384\n",
      "precision class 1 = 0.8256410360336304\n",
      "recall class 0 = 0.9909090995788574\n",
      "recall class 1 = 0.5494880676269531\n",
      "AUC of ROC = 0.9391387271632203\n",
      "AUC of PRC = 0.7515773743569496\n",
      "min(+P, Se) = 0.6791808873720137\n",
      "f1_score = 0.6598360782794032\n",
      "\n",
      "Epoch 19 Batch 0: Train Loss = 0.1576\n",
      "Epoch 19 Batch 30: Train Loss = 0.1319\n",
      "Epoch 19 Batch 60: Train Loss = 0.1330\n",
      "Epoch 19 Batch 90: Train Loss = 0.1283\n",
      "Epoch 19 Batch 120: Train Loss = 0.1273\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1285\n",
      "valid_model Loss = 0.1285\n",
      "confusion matrix:\n",
      "[[3713   27]\n",
      " [ 132  161]]\n",
      "accuracy = 0.9605752825737\n",
      "precision class 0 = 0.9656696915626526\n",
      "precision class 1 = 0.8563829660415649\n",
      "recall class 0 = 0.9927807450294495\n",
      "recall class 1 = 0.5494880676269531\n",
      "AUC of ROC = 0.9417997481338176\n",
      "AUC of PRC = 0.7509356487290385\n",
      "min(+P, Se) = 0.6860068259385665\n",
      "f1_score = 0.6694386752259831\n",
      "\n",
      "Epoch 20 Batch 0: Train Loss = 0.1506\n",
      "Epoch 20 Batch 30: Train Loss = 0.1279\n",
      "Epoch 20 Batch 60: Train Loss = 0.1296\n",
      "Epoch 20 Batch 90: Train Loss = 0.1258\n",
      "Epoch 20 Batch 120: Train Loss = 0.1238\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1259\n",
      "valid_model Loss = 0.1259\n",
      "confusion matrix:\n",
      "[[3709   31]\n",
      " [ 127  166]]\n",
      "accuracy = 0.9608232378959656\n",
      "precision class 0 = 0.9668926000595093\n",
      "precision class 1 = 0.8426395654678345\n",
      "recall class 0 = 0.9917112588882446\n",
      "recall class 1 = 0.5665528774261475\n",
      "AUC of ROC = 0.9425352703911226\n",
      "AUC of PRC = 0.7630203851588352\n",
      "min(+P, Se) = 0.689419795221843\n",
      "f1_score = 0.6775509943390131\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 21 Batch 0: Train Loss = 0.1393\n",
      "Epoch 21 Batch 30: Train Loss = 0.1188\n",
      "Epoch 21 Batch 60: Train Loss = 0.1220\n",
      "Epoch 21 Batch 90: Train Loss = 0.1222\n",
      "Epoch 21 Batch 120: Train Loss = 0.1221\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1238\n",
      "valid_model Loss = 0.1238\n",
      "confusion matrix:\n",
      "[[3702   38]\n",
      " [ 124  169]]\n",
      "accuracy = 0.9598314166069031\n",
      "precision class 0 = 0.9675901532173157\n",
      "precision class 1 = 0.8164251446723938\n",
      "recall class 0 = 0.9898395538330078\n",
      "recall class 1 = 0.5767918229103088\n",
      "AUC of ROC = 0.943301819641912\n",
      "AUC of PRC = 0.754238898850817\n",
      "min(+P, Se) = 0.6996587030716723\n",
      "f1_score = 0.676000017832756\n",
      "\n",
      "Epoch 22 Batch 0: Train Loss = 0.1245\n",
      "Epoch 22 Batch 30: Train Loss = 0.1312\n",
      "Epoch 22 Batch 60: Train Loss = 0.1281\n",
      "Epoch 22 Batch 90: Train Loss = 0.1228\n",
      "Epoch 22 Batch 120: Train Loss = 0.1224\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1198\n",
      "valid_model Loss = 0.1198\n",
      "confusion matrix:\n",
      "[[3700   40]\n",
      " [ 119  174]]\n",
      "accuracy = 0.9605752825737\n",
      "precision class 0 = 0.9688400030136108\n",
      "precision class 1 = 0.8130841255187988\n",
      "recall class 0 = 0.9893048405647278\n",
      "recall class 1 = 0.5938566327095032\n",
      "AUC of ROC = 0.9449772772900659\n",
      "AUC of PRC = 0.7696121033684313\n",
      "min(+P, Se) = 0.7098976109215017\n",
      "f1_score = 0.6863905513039735\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 23 Batch 0: Train Loss = 0.1140\n",
      "Epoch 23 Batch 30: Train Loss = 0.1267\n",
      "Epoch 23 Batch 60: Train Loss = 0.1247\n",
      "Epoch 23 Batch 90: Train Loss = 0.1192\n",
      "Epoch 23 Batch 120: Train Loss = 0.1199\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1216\n",
      "valid_model Loss = 0.1216\n",
      "confusion matrix:\n",
      "[[3688   52]\n",
      " [ 111  182]]\n",
      "accuracy = 0.9595834612846375\n",
      "precision class 0 = 0.9707818031311035\n",
      "precision class 1 = 0.7777777910232544\n",
      "recall class 0 = 0.9860962629318237\n",
      "recall class 1 = 0.6211603879928589\n",
      "AUC of ROC = 0.942472303845522\n",
      "AUC of PRC = 0.7646743131153559\n",
      "min(+P, Se) = 0.7040816326530612\n",
      "f1_score = 0.6907020791783948\n",
      "\n",
      "Epoch 24 Batch 0: Train Loss = 0.1559\n",
      "Epoch 24 Batch 30: Train Loss = 0.1272\n",
      "Epoch 24 Batch 60: Train Loss = 0.1247\n",
      "Epoch 24 Batch 90: Train Loss = 0.1227\n",
      "Epoch 24 Batch 120: Train Loss = 0.1224\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1180\n",
      "valid_model Loss = 0.1180\n",
      "confusion matrix:\n",
      "[[3695   45]\n",
      " [ 113  180]]\n",
      "accuracy = 0.9608232378959656\n",
      "precision class 0 = 0.9703256487846375\n",
      "precision class 1 = 0.800000011920929\n",
      "recall class 0 = 0.9879679083824158\n",
      "recall class 1 = 0.6143344640731812\n",
      "AUC of ROC = 0.9440565056304868\n",
      "AUC of PRC = 0.7620652502718195\n",
      "min(+P, Se) = 0.7030716723549488\n",
      "f1_score = 0.6949806657643708\n",
      "\n",
      "Epoch 25 Batch 0: Train Loss = 0.1721\n",
      "Epoch 25 Batch 30: Train Loss = 0.1260\n",
      "Epoch 25 Batch 60: Train Loss = 0.1224\n",
      "Epoch 25 Batch 90: Train Loss = 0.1241\n",
      "Epoch 25 Batch 120: Train Loss = 0.1204\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1253\n",
      "valid_model Loss = 0.1253\n",
      "confusion matrix:\n",
      "[[3717   23]\n",
      " [ 141  152]]\n",
      "accuracy = 0.9593355059623718\n",
      "precision class 0 = 0.9634525775909424\n",
      "precision class 1 = 0.868571400642395\n",
      "recall class 0 = 0.9938502907752991\n",
      "recall class 1 = 0.5187713503837585\n",
      "AUC of ROC = 0.9424120749758172\n",
      "AUC of PRC = 0.7639920213849372\n",
      "min(+P, Se) = 0.7016949152542373\n",
      "f1_score = 0.6495726848199002\n",
      "\n",
      "Epoch 26 Batch 0: Train Loss = 0.1374\n",
      "Epoch 26 Batch 30: Train Loss = 0.1245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Batch 60: Train Loss = 0.1168\n",
      "Epoch 26 Batch 90: Train Loss = 0.1201\n",
      "Epoch 26 Batch 120: Train Loss = 0.1208\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1230\n",
      "valid_model Loss = 0.1230\n",
      "confusion matrix:\n",
      "[[3707   33]\n",
      " [ 133  160]]\n",
      "accuracy = 0.9588395953178406\n",
      "precision class 0 = 0.9653645753860474\n",
      "precision class 1 = 0.8290155529975891\n",
      "recall class 0 = 0.9911764860153198\n",
      "recall class 1 = 0.5460751056671143\n",
      "AUC of ROC = 0.9411646073260207\n",
      "AUC of PRC = 0.7464789368283604\n",
      "min(+P, Se) = 0.673469387755102\n",
      "f1_score = 0.6584362601450193\n",
      "\n",
      "Epoch 27 Batch 0: Train Loss = 0.1120\n",
      "Epoch 27 Batch 30: Train Loss = 0.1249\n",
      "Epoch 27 Batch 60: Train Loss = 0.1124\n",
      "Epoch 27 Batch 90: Train Loss = 0.1168\n",
      "Epoch 27 Batch 120: Train Loss = 0.1182\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1252\n",
      "valid_model Loss = 0.1252\n",
      "confusion matrix:\n",
      "[[3722   18]\n",
      " [ 141  152]]\n",
      "accuracy = 0.9605752825737\n",
      "precision class 0 = 0.9634998440742493\n",
      "precision class 1 = 0.8941176533699036\n",
      "recall class 0 = 0.9951871633529663\n",
      "recall class 1 = 0.5187713503837585\n",
      "AUC of ROC = 0.9447190231972404\n",
      "AUC of PRC = 0.7632988993033215\n",
      "min(+P, Se) = 0.7064846416382252\n",
      "f1_score = 0.6565874901826801\n",
      "\n",
      "Epoch 28 Batch 0: Train Loss = 0.1076\n",
      "Epoch 28 Batch 30: Train Loss = 0.1185\n",
      "Epoch 28 Batch 60: Train Loss = 0.1211\n",
      "Epoch 28 Batch 90: Train Loss = 0.1193\n",
      "Epoch 28 Batch 120: Train Loss = 0.1190\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1173\n",
      "valid_model Loss = 0.1173\n",
      "confusion matrix:\n",
      "[[3709   31]\n",
      " [ 122  171]]\n",
      "accuracy = 0.9620629549026489\n",
      "precision class 0 = 0.9681545495986938\n",
      "precision class 1 = 0.8465346693992615\n",
      "recall class 0 = 0.9917112588882446\n",
      "recall class 1 = 0.5836177468299866\n",
      "AUC of ROC = 0.9473864320782611\n",
      "AUC of PRC = 0.7700461576077112\n",
      "min(+P, Se) = 0.7030716723549488\n",
      "f1_score = 0.6909090957883888\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 29 Batch 0: Train Loss = 0.0896\n",
      "Epoch 29 Batch 30: Train Loss = 0.1274\n",
      "Epoch 29 Batch 60: Train Loss = 0.1238\n",
      "Epoch 29 Batch 90: Train Loss = 0.1244\n",
      "Epoch 29 Batch 120: Train Loss = 0.1192\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1232\n",
      "valid_model Loss = 0.1232\n",
      "confusion matrix:\n",
      "[[3692   48]\n",
      " [ 106  187]]\n",
      "accuracy = 0.9618149995803833\n",
      "precision class 0 = 0.9720906019210815\n",
      "precision class 1 = 0.7957446575164795\n",
      "recall class 0 = 0.9871657490730286\n",
      "recall class 1 = 0.638225257396698\n",
      "AUC of ROC = 0.9459455019985036\n",
      "AUC of PRC = 0.7689697091759224\n",
      "min(+P, Se) = 0.7201365187713311\n",
      "f1_score = 0.7083332955228351\n",
      "\n",
      "Epoch 30 Batch 0: Train Loss = 0.1027\n",
      "Epoch 30 Batch 30: Train Loss = 0.1195\n",
      "Epoch 30 Batch 60: Train Loss = 0.1179\n",
      "Epoch 30 Batch 90: Train Loss = 0.1169\n",
      "Epoch 30 Batch 120: Train Loss = 0.1176\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1226\n",
      "valid_model Loss = 0.1226\n",
      "confusion matrix:\n",
      "[[3706   34]\n",
      " [ 129  164]]\n",
      "accuracy = 0.9595834612846375\n",
      "precision class 0 = 0.966362476348877\n",
      "precision class 1 = 0.8282828330993652\n",
      "recall class 0 = 0.9909090995788574\n",
      "recall class 1 = 0.5597269535064697\n",
      "AUC of ROC = 0.9427214323520288\n",
      "AUC of PRC = 0.7590621048336802\n",
      "min(+P, Se) = 0.6996587030716723\n",
      "f1_score = 0.6680244351102279\n",
      "\n",
      "Epoch 31 Batch 0: Train Loss = 0.0937\n",
      "Epoch 31 Batch 30: Train Loss = 0.1110\n",
      "Epoch 31 Batch 60: Train Loss = 0.1104\n",
      "Epoch 31 Batch 90: Train Loss = 0.1106\n",
      "Epoch 31 Batch 120: Train Loss = 0.1159\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1192\n",
      "valid_model Loss = 0.1192\n",
      "confusion matrix:\n",
      "[[3705   35]\n",
      " [ 118  175]]\n",
      "accuracy = 0.9620629549026489\n",
      "precision class 0 = 0.9691342115402222\n",
      "precision class 1 = 0.8333333134651184\n",
      "recall class 0 = 0.990641713142395\n",
      "recall class 1 = 0.5972696542739868\n",
      "AUC of ROC = 0.9450511945392492\n",
      "AUC of PRC = 0.7630463672249528\n",
      "min(+P, Se) = 0.6962457337883959\n",
      "f1_score = 0.6958250339403375\n",
      "\n",
      "Epoch 32 Batch 0: Train Loss = 0.0800\n",
      "Epoch 32 Batch 30: Train Loss = 0.1235\n",
      "Epoch 32 Batch 60: Train Loss = 0.1184\n",
      "Epoch 32 Batch 90: Train Loss = 0.1154\n",
      "Epoch 32 Batch 120: Train Loss = 0.1149\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1171\n",
      "valid_model Loss = 0.1171\n",
      "confusion matrix:\n",
      "[[3712   28]\n",
      " [ 128  165]]\n",
      "accuracy = 0.961319088935852\n",
      "precision class 0 = 0.9666666388511658\n",
      "precision class 1 = 0.8549222946166992\n",
      "recall class 0 = 0.9925133585929871\n",
      "recall class 1 = 0.5631399154663086\n",
      "AUC of ROC = 0.9472203464072567\n",
      "AUC of PRC = 0.7740664885734251\n",
      "min(+P, Se) = 0.7303754266211604\n",
      "f1_score = 0.6790123385243146\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 33 Batch 0: Train Loss = 0.1089\n",
      "Epoch 33 Batch 30: Train Loss = 0.1070\n",
      "Epoch 33 Batch 60: Train Loss = 0.1115\n",
      "Epoch 33 Batch 90: Train Loss = 0.1144\n",
      "Epoch 33 Batch 120: Train Loss = 0.1151\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1140\n",
      "valid_model Loss = 0.1140\n",
      "confusion matrix:\n",
      "[[3696   44]\n",
      " [ 109  184]]\n",
      "accuracy = 0.9620629549026489\n",
      "precision class 0 = 0.9713534712791443\n",
      "precision class 1 = 0.8070175647735596\n",
      "recall class 0 = 0.9882352948188782\n",
      "recall class 1 = 0.6279863715171814\n",
      "AUC of ROC = 0.9467622419740469\n",
      "AUC of PRC = 0.7737616503007376\n",
      "min(+P, Se) = 0.7235494880546075\n",
      "f1_score = 0.7063339665985514\n",
      "\n",
      "Epoch 34 Batch 0: Train Loss = 0.0809\n",
      "Epoch 34 Batch 30: Train Loss = 0.1151\n",
      "Epoch 34 Batch 60: Train Loss = 0.1152\n",
      "Epoch 34 Batch 90: Train Loss = 0.1148\n",
      "Epoch 34 Batch 120: Train Loss = 0.1142\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1192\n",
      "valid_model Loss = 0.1192\n",
      "confusion matrix:\n",
      "[[3703   37]\n",
      " [ 107  186]]\n",
      "accuracy = 0.9642945528030396\n",
      "precision class 0 = 0.9719160199165344\n",
      "precision class 1 = 0.834080696105957\n",
      "recall class 0 = 0.9901069402694702\n",
      "recall class 1 = 0.6348122954368591\n",
      "AUC of ROC = 0.9462840612509354\n",
      "AUC of PRC = 0.7713950607068467\n",
      "min(+P, Se) = 0.6996587030716723\n",
      "f1_score = 0.720930200957757\n",
      "\n",
      "Epoch 35 Batch 0: Train Loss = 0.0744\n",
      "Epoch 35 Batch 30: Train Loss = 0.1129\n",
      "Epoch 35 Batch 60: Train Loss = 0.1156\n",
      "Epoch 35 Batch 90: Train Loss = 0.1152\n",
      "Epoch 35 Batch 120: Train Loss = 0.1145\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1175\n",
      "valid_model Loss = 0.1175\n",
      "confusion matrix:\n",
      "[[3697   43]\n",
      " [ 107  186]]\n",
      "accuracy = 0.9628068208694458\n",
      "precision class 0 = 0.9718717336654663\n",
      "precision class 1 = 0.8122270703315735\n",
      "recall class 0 = 0.9885026812553406\n",
      "recall class 1 = 0.6348122954368591\n",
      "AUC of ROC = 0.9465988939789381\n",
      "AUC of PRC = 0.7687418254391762\n",
      "min(+P, Se) = 0.7006802721088435\n",
      "f1_score = 0.7126436821700868\n",
      "\n",
      "Epoch 36 Batch 0: Train Loss = 0.1338\n",
      "Epoch 36 Batch 30: Train Loss = 0.1151\n",
      "Epoch 36 Batch 60: Train Loss = 0.1166\n",
      "Epoch 36 Batch 90: Train Loss = 0.1176\n",
      "Epoch 36 Batch 120: Train Loss = 0.1168\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1174\n",
      "valid_model Loss = 0.1174\n",
      "confusion matrix:\n",
      "[[3721   19]\n",
      " [ 134  159]]\n",
      "accuracy = 0.9620629549026489\n",
      "precision class 0 = 0.965239942073822\n",
      "precision class 1 = 0.8932584524154663\n",
      "recall class 0 = 0.9949197769165039\n",
      "recall class 1 = 0.5426621437072754\n",
      "AUC of ROC = 0.9491002171889543\n",
      "AUC of PRC = 0.7790739228690305\n",
      "min(+P, Se) = 0.7064846416382252\n",
      "f1_score = 0.675159264351112\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 37 Batch 0: Train Loss = 0.1162\n",
      "Epoch 37 Batch 30: Train Loss = 0.1124\n",
      "Epoch 37 Batch 60: Train Loss = 0.1121\n",
      "Epoch 37 Batch 90: Train Loss = 0.1122\n",
      "Epoch 37 Batch 120: Train Loss = 0.1138\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1229\n",
      "valid_model Loss = 0.1229\n",
      "confusion matrix:\n",
      "[[3667   73]\n",
      " [  97  196]]\n",
      "accuracy = 0.9578477740287781\n",
      "precision class 0 = 0.9742295145988464\n",
      "precision class 1 = 0.7286245226860046\n",
      "recall class 0 = 0.9804812669754028\n",
      "recall class 1 = 0.6689419746398926\n",
      "AUC of ROC = 0.9427241700279244\n",
      "AUC of PRC = 0.7596953975818819\n",
      "min(+P, Se) = 0.7030716723549488\n",
      "f1_score = 0.6975088586079395\n",
      "\n",
      "Epoch 38 Batch 0: Train Loss = 0.0974\n",
      "Epoch 38 Batch 30: Train Loss = 0.1157\n",
      "Epoch 38 Batch 60: Train Loss = 0.1189\n",
      "Epoch 38 Batch 90: Train Loss = 0.1168\n",
      "Epoch 38 Batch 120: Train Loss = 0.1151\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1300\n",
      "valid_model Loss = 0.1300\n",
      "confusion matrix:\n",
      "[[3724   16]\n",
      " [ 140  153]]\n",
      "accuracy = 0.961319088935852\n",
      "precision class 0 = 0.9637681245803833\n",
      "precision class 1 = 0.9053254723548889\n",
      "recall class 0 = 0.9957219362258911\n",
      "recall class 1 = 0.5221843123435974\n",
      "AUC of ROC = 0.9411673450019165\n",
      "AUC of PRC = 0.7521430351681927\n",
      "min(+P, Se) = 0.6621160409556314\n",
      "f1_score = 0.6623376796378767\n",
      "\n",
      "Epoch 39 Batch 0: Train Loss = 0.0962\n",
      "Epoch 39 Batch 30: Train Loss = 0.1091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Batch 60: Train Loss = 0.1104\n",
      "Epoch 39 Batch 90: Train Loss = 0.1103\n",
      "Epoch 39 Batch 120: Train Loss = 0.1113\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1136\n",
      "valid_model Loss = 0.1136\n",
      "confusion matrix:\n",
      "[[3714   26]\n",
      " [ 119  174]]\n",
      "accuracy = 0.9640465974807739\n",
      "precision class 0 = 0.9689538478851318\n",
      "precision class 1 = 0.8700000047683716\n",
      "recall class 0 = 0.9930481314659119\n",
      "recall class 1 = 0.5938566327095032\n",
      "AUC of ROC = 0.9501487470569985\n",
      "AUC of PRC = 0.7796814264063661\n",
      "min(+P, Se) = 0.7220338983050848\n",
      "f1_score = 0.7058823098172051\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 40 Batch 0: Train Loss = 0.1683\n",
      "Epoch 40 Batch 30: Train Loss = 0.1114\n",
      "Epoch 40 Batch 60: Train Loss = 0.1082\n",
      "Epoch 40 Batch 90: Train Loss = 0.1120\n",
      "Epoch 40 Batch 120: Train Loss = 0.1127\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1172\n",
      "valid_model Loss = 0.1172\n",
      "confusion matrix:\n",
      "[[3687   53]\n",
      " [ 101  192]]\n",
      "accuracy = 0.9618149995803833\n",
      "precision class 0 = 0.9733368754386902\n",
      "precision class 1 = 0.7836734652519226\n",
      "recall class 0 = 0.9858288764953613\n",
      "recall class 1 = 0.6552901268005371\n",
      "AUC of ROC = 0.9475338102973118\n",
      "AUC of PRC = 0.7706262854787139\n",
      "min(+P, Se) = 0.7133105802047781\n",
      "f1_score = 0.7137546300405374\n",
      "\n",
      "Epoch 41 Batch 0: Train Loss = 0.0918\n",
      "Epoch 41 Batch 30: Train Loss = 0.1117\n",
      "Epoch 41 Batch 60: Train Loss = 0.1163\n",
      "Epoch 41 Batch 90: Train Loss = 0.1182\n",
      "Epoch 41 Batch 120: Train Loss = 0.1143\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1176\n",
      "valid_model Loss = 0.1176\n",
      "confusion matrix:\n",
      "[[3700   40]\n",
      " [ 114  179]]\n",
      "accuracy = 0.9618149995803833\n",
      "precision class 0 = 0.9701101183891296\n",
      "precision class 1 = 0.8173515796661377\n",
      "recall class 0 = 0.9893048405647278\n",
      "recall class 1 = 0.6109215021133423\n",
      "AUC of ROC = 0.9454636710408644\n",
      "AUC of PRC = 0.7753441888972173\n",
      "min(+P, Se) = 0.7201365187713311\n",
      "f1_score = 0.6992187434943843\n",
      "\n",
      "Epoch 42 Batch 0: Train Loss = 0.1177\n",
      "Epoch 42 Batch 30: Train Loss = 0.1009\n",
      "Epoch 42 Batch 60: Train Loss = 0.1094\n",
      "Epoch 42 Batch 90: Train Loss = 0.1089\n",
      "Epoch 42 Batch 120: Train Loss = 0.1116\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1274\n",
      "valid_model Loss = 0.1274\n",
      "confusion matrix:\n",
      "[[3727   13]\n",
      " [ 143  150]]\n",
      "accuracy = 0.961319088935852\n",
      "precision class 0 = 0.9630491137504578\n",
      "precision class 1 = 0.9202454090118408\n",
      "recall class 0 = 0.9965240359306335\n",
      "recall class 1 = 0.511945366859436\n",
      "AUC of ROC = 0.9450229052216605\n",
      "AUC of PRC = 0.767224329638776\n",
      "min(+P, Se) = 0.7167235494880546\n",
      "f1_score = 0.6578947182936218\n",
      "\n",
      "Epoch 43 Batch 0: Train Loss = 0.1095\n",
      "Epoch 43 Batch 30: Train Loss = 0.1080\n",
      "Epoch 43 Batch 60: Train Loss = 0.1104\n",
      "Epoch 43 Batch 90: Train Loss = 0.1115\n",
      "Epoch 43 Batch 120: Train Loss = 0.1115\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1175\n",
      "valid_model Loss = 0.1175\n",
      "confusion matrix:\n",
      "[[3703   37]\n",
      " [ 114  179]]\n",
      "accuracy = 0.9625588655471802\n",
      "precision class 0 = 0.9701336026191711\n",
      "precision class 1 = 0.8287037014961243\n",
      "recall class 0 = 0.9901069402694702\n",
      "recall class 1 = 0.6109215021133423\n",
      "AUC of ROC = 0.948265682320089\n",
      "AUC of PRC = 0.7770442795600336\n",
      "min(+P, Se) = 0.7201365187713311\n",
      "f1_score = 0.7033398524760434\n",
      "\n",
      "Epoch 44 Batch 0: Train Loss = 0.0784\n",
      "Epoch 44 Batch 30: Train Loss = 0.1066\n",
      "Epoch 44 Batch 60: Train Loss = 0.1118\n",
      "Epoch 44 Batch 90: Train Loss = 0.1127\n",
      "Epoch 44 Batch 120: Train Loss = 0.1115\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1204\n",
      "valid_model Loss = 0.1204\n",
      "confusion matrix:\n",
      "[[3721   19]\n",
      " [ 131  162]]\n",
      "accuracy = 0.9628068208694458\n",
      "precision class 0 = 0.9659916758537292\n",
      "precision class 1 = 0.8950276374816895\n",
      "recall class 0 = 0.9949197769165039\n",
      "recall class 1 = 0.552901029586792\n",
      "AUC of ROC = 0.9500921684218211\n",
      "AUC of PRC = 0.7802577765225052\n",
      "min(+P, Se) = 0.717687074829932\n",
      "f1_score = 0.6835443119917938\n",
      "\n",
      "\n",
      "------------ Save best-prc model ------------\n",
      "\n",
      "Epoch 45 Batch 0: Train Loss = 0.1391\n",
      "Epoch 45 Batch 30: Train Loss = 0.1117\n",
      "Epoch 45 Batch 60: Train Loss = 0.1098\n",
      "Epoch 45 Batch 90: Train Loss = 0.1093\n",
      "Epoch 45 Batch 120: Train Loss = 0.1106\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1221\n",
      "valid_model Loss = 0.1221\n",
      "confusion matrix:\n",
      "[[3712   28]\n",
      " [ 124  169]]\n",
      "accuracy = 0.9623109102249146\n",
      "precision class 0 = 0.9676746726036072\n",
      "precision class 1 = 0.8578680157661438\n",
      "recall class 0 = 0.9925133585929871\n",
      "recall class 1 = 0.5767918229103088\n",
      "AUC of ROC = 0.9452957602525961\n",
      "AUC of PRC = 0.7630291251175347\n",
      "min(+P, Se) = 0.6928327645051194\n",
      "f1_score = 0.6897959269379039\n",
      "\n",
      "Epoch 46 Batch 0: Train Loss = 0.1316\n",
      "Epoch 46 Batch 30: Train Loss = 0.1083\n",
      "Epoch 46 Batch 60: Train Loss = 0.1089\n",
      "Epoch 46 Batch 90: Train Loss = 0.1100\n",
      "Epoch 46 Batch 120: Train Loss = 0.1101\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1190\n",
      "valid_model Loss = 0.1190\n",
      "confusion matrix:\n",
      "[[3716   24]\n",
      " [ 124  169]]\n",
      "accuracy = 0.963302731513977\n",
      "precision class 0 = 0.9677083492279053\n",
      "precision class 1 = 0.8756476640701294\n",
      "recall class 0 = 0.9935829043388367\n",
      "recall class 1 = 0.5767918229103088\n",
      "AUC of ROC = 0.9475041521417751\n",
      "AUC of PRC = 0.778680478026894\n",
      "min(+P, Se) = 0.7098976109215017\n",
      "f1_score = 0.6954732313281545\n",
      "\n",
      "Epoch 47 Batch 0: Train Loss = 0.0920\n",
      "Epoch 47 Batch 30: Train Loss = 0.1154\n",
      "Epoch 47 Batch 60: Train Loss = 0.1112\n",
      "Epoch 47 Batch 90: Train Loss = 0.1129\n",
      "Epoch 47 Batch 120: Train Loss = 0.1106\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1214\n",
      "valid_model Loss = 0.1214\n",
      "confusion matrix:\n",
      "[[3666   74]\n",
      " [  94  199]]\n",
      "accuracy = 0.9583436846733093\n",
      "precision class 0 = 0.9750000238418579\n",
      "precision class 1 = 0.7289377450942993\n",
      "recall class 0 = 0.9802138805389404\n",
      "recall class 1 = 0.6791808605194092\n",
      "AUC of ROC = 0.9485198298990711\n",
      "AUC of PRC = 0.7773566882674464\n",
      "min(+P, Se) = 0.7064846416382252\n",
      "f1_score = 0.7031802051396802\n",
      "\n",
      "Epoch 48 Batch 0: Train Loss = 0.1017\n",
      "Epoch 48 Batch 30: Train Loss = 0.1155\n",
      "Epoch 48 Batch 60: Train Loss = 0.1115\n",
      "Epoch 48 Batch 90: Train Loss = 0.1102\n",
      "Epoch 48 Batch 120: Train Loss = 0.1091\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1172\n",
      "valid_model Loss = 0.1172\n",
      "confusion matrix:\n",
      "[[3691   49]\n",
      " [ 108  185]]\n",
      "accuracy = 0.9610711336135864\n",
      "precision class 0 = 0.9715714454650879\n",
      "precision class 1 = 0.7905982732772827\n",
      "recall class 0 = 0.9868984222412109\n",
      "recall class 1 = 0.6313993334770203\n",
      "AUC of ROC = 0.9466782865799128\n",
      "AUC of PRC = 0.7696625486397088\n",
      "min(+P, Se) = 0.7074829931972789\n",
      "f1_score = 0.7020873190617717\n",
      "\n",
      "Epoch 49 Batch 0: Train Loss = 0.1480\n",
      "Epoch 49 Batch 30: Train Loss = 0.1055\n",
      "Epoch 49 Batch 60: Train Loss = 0.1099\n",
      "Epoch 49 Batch 90: Train Loss = 0.1118\n",
      "Epoch 49 Batch 120: Train Loss = 0.1095\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.1170\n",
      "valid_model Loss = 0.1170\n",
      "confusion matrix:\n",
      "[[3707   33]\n",
      " [ 125  168]]\n",
      "accuracy = 0.9608232378959656\n",
      "precision class 0 = 0.9673799872398376\n",
      "precision class 1 = 0.8358209133148193\n",
      "recall class 0 = 0.9911764860153198\n",
      "recall class 1 = 0.57337886095047\n",
      "AUC of ROC = 0.9498722417915351\n",
      "AUC of PRC = 0.77754881623754\n",
      "min(+P, Se) = 0.7166666666666667\n",
      "f1_score = 0.6801619930081251\n",
      "\n",
      "==============DONE==============\n"
     ]
    }
   ],
   "source": [
    "file_name = './model/MAPLE-100-20-model'\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "pad_token = np.zeros(33)\n",
    "max_roc = 0\n",
    "max_prc = 0\n",
    "train_loss = []\n",
    "train_model_loss = []\n",
    "# train_decov_loss = []\n",
    "valid_loss = []\n",
    "valid_model_loss = []\n",
    "# valid_decov_loss = []\n",
    "history = []\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(precision=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    counter_batch = 0\n",
    "    batch_loss = []\n",
    "    model_batch_loss = []\n",
    "\n",
    "    epoch_loss = []\n",
    "    counter_batch = 0\n",
    "    model.train() \n",
    "    \n",
    "    for step, (batch_x, batch_y, batch_lens) in enumerate(batch_iter(train_x, train_y, train_x_len, batch_size, shuffle=True)):  \n",
    "\n",
    "        counter_batch += len(batch_x)\n",
    "        optimizer.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "        if batch_x.shape[0] < cluster_num:\n",
    "            continue\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "        opt, _ = model(batch_x, each_epoch, batch_lens)\n",
    "        \n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "\n",
    "        loss = model_loss\n",
    "        \n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        model_batch_loss.append(model_loss.cpu().detach().numpy())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 30 == 0:\n",
    "            print('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, np.mean(np.array(batch_loss))))\n",
    "    train_loss.append(np.mean(np.array(batch_loss)))\n",
    "    train_model_loss.append(np.mean(np.array(model_batch_loss)))\n",
    "\n",
    "    \n",
    "\n",
    "    batch_loss = []\n",
    "    model_batch_loss = []\n",
    "    \n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_x, batch_y, batch_lens in batch_iter(dev_x, dev_y, dev_x_len, 256):\n",
    "            batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "            if batch_x.shape[0] < cluster_num:\n",
    "                continue\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "            masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "     \n",
    "            opt, _ = model(batch_x, 100, batch_lens)\n",
    "        \n",
    "            model_loss = get_loss(opt, batch_y.unsqueeze(-1)) \n",
    "\n",
    "\n",
    "            loss = model_loss\n",
    "            \n",
    "            batch_loss.append(loss.cpu().detach().numpy())\n",
    "            model_batch_loss.append(model_loss.cpu().detach().numpy())\n",
    "            y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "            y_true += list(batch_y.cpu().numpy().flatten())\n",
    "            \n",
    "    valid_loss.append(np.mean(np.array(batch_loss)))\n",
    "    valid_model_loss.append(np.mean(np.array(model_batch_loss)))\n",
    "    \n",
    "    print(\"\\n==>Predicting on validation\")\n",
    "    print('Valid Loss = %.4f'%(valid_loss[-1]))\n",
    "    print('valid_model Loss = %.4f'%(valid_model_loss[-1]))\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "    ret = metrics.print_metrics_binary(y_true, y_pred)\n",
    "    history.append(ret)\n",
    "    print('')\n",
    "\n",
    "    \n",
    "    cur_auroc = ret['auroc']\n",
    "    cur_prc = ret['auprc']\n",
    "    \n",
    "    if cur_prc > max_prc:\n",
    "        max_prc = cur_prc\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': each_epoch\n",
    "        }\n",
    "        torch.save(state, file_name+\"prc\")\n",
    "        print('\\n------------ Save best-prc model ------------\\n')\n",
    "        \n",
    "print('==============DONE==============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(file_name)\n",
    "save_epoch = checkpoint['epoch']\n",
    "print(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-25T14:38:58.904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last saved model is in epoch 32\n",
      "Batch 0: Test Loss = 0.1203\n",
      "\n",
      "==>Predicting on test\n",
      "Test Loss = 0.1066\n",
      "confusion matrix:\n",
      "[[3698   43]\n",
      " [ 103  191]]\n",
      "accuracy = 0.9638165831565857\n",
      "precision class 0 = 0.9729018807411194\n",
      "precision class 1 = 0.8162392973899841\n",
      "recall class 0 = 0.9885057210922241\n",
      "recall class 1 = 0.6496598720550537\n",
      "AUC of ROC = 0.9567651706499225\n",
      "AUC of PRC = 0.8044356173079956\n",
      "min(+P, Se) = 0.7210884353741497\n",
      "f1_score = 0.7234848166916005\n"
     ]
    }
   ],
   "source": [
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for step, (batch_x, batch_y, batch_lens) in enumerate(batch_iter(test_x, test_y, test_x_len, batch_size, shuffle=False)):  \n",
    "        optimizer.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#         batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "        opt,_ = model(batch_x, 100, batch_lens)\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "\n",
    "        loss = model_loss\n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "        y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "        y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "test_res = metrics.print_metrics_binary(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T05:39:59.022411Z",
     "start_time": "2020-05-22T05:39:45.542399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Test Loss = 0.1203\n",
      "\n",
      "==>Predicting on test\n",
      "Test Loss = 0.1066\n",
      "confusion matrix:\n",
      "[[3698   43]\n",
      " [ 103  191]]\n",
      "accuracy = 0.9638165831565857\n",
      "precision class 0 = 0.9729018807411194\n",
      "precision class 1 = 0.8162392973899841\n",
      "recall class 0 = 0.9885057210922241\n",
      "recall class 1 = 0.6496598720550537\n",
      "AUC of ROC = 0.9567651706499225\n",
      "AUC of PRC = 0.8044356173079956\n",
      "min(+P, Se) = 0.7210884353741497\n",
      "f1_score = 0.7234848166916005\n"
     ]
    }
   ],
   "source": [
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for step, (batch_x, batch_y, batch_lens) in enumerate(batch_iter(test_x, test_y, test_x_len, batch_size, shuffle=False)):  \n",
    "        optimizer.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#         batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "        opt,_ = model(batch_x, 100, batch_lens)\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "\n",
    "        loss = model_loss\n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "        y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "        y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "test_res = metrics.print_metrics_binary(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T05:41:47.299532Z",
     "start_time": "2020-05-22T05:41:28.484540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc 0.9565(0.0064)\n",
      "auprc 0.8043(0.0201)\n",
      "minpse 0.7213(0.0214)\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap\n",
    "N = len(y_true)\n",
    "N_idx = np.arange(N)\n",
    "K = 1000\n",
    "\n",
    "auroc = []\n",
    "auprc = []\n",
    "minpse = []\n",
    "for i in range(K):\n",
    "    boot_idx = np.random.choice(N_idx, N, replace=True)\n",
    "    boot_true = np.array(y_true)[boot_idx]\n",
    "    boot_pred = y_pred[boot_idx, :]\n",
    "    test_ret = metrics.print_metrics_binary(boot_true, boot_pred, verbose=0)\n",
    "    auroc.append(test_ret['auroc'])\n",
    "    auprc.append(test_ret['auprc'])\n",
    "    minpse.append(test_ret['minpse'])\n",
    "#     print('%d/%d'%(i+1,K))\n",
    "    \n",
    "print('auroc %.4f(%.4f)'%(np.mean(auroc), np.std(auroc)))\n",
    "print('auprc %.4f(%.4f)'%(np.mean(auprc), np.std(auprc)))\n",
    "print('minpse %.4f(%.4f)'%(np.mean(minpse), np.std(minpse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
